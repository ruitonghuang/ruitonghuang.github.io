<!DOCTYPE html><html><head><meta charset="utf-8"><title>index 2.html</title><style></style></head><body id="preview">
<h1 class="code-line" data-line-start=0 data-line-end=1 ><a id="Ruitong_Huang_0"></a>Ruitong Huang</h1>
<h5 class="code-line" data-line-start=2 data-line-end=3 ><a id="Borealis_AI_Canada_2"></a>Borealis AI, Canada</h5>
<h5 class="code-line" data-line-start=4 data-line-end=5 ><a id="first_name_at_ualbertaca__rtonglast_name_at_gmailcom__first_namelastname_at_borealisaicom_4"></a>[first name] at <a href="http://ualberta.ca">ualberta.ca</a> / rtong[last name] at <a href="http://gmail.com">gmail.com</a> / [first name].[lastname] at <a href="http://borealisai.com">borealisai.com</a></h5>
<hr>
<p class="has-line-data" data-line-start="8" data-line-end="9">I am now a research team lead at Borealis AI, Canada. My research focuses on studying and designing provably efficient machine learning algorithms. The topics that I have worked on include Online Learning, Reinforcement Learning, Adversarial Learning/Robustness, and Statistical Learning Theory. My resume is <a href="http://webdocs.cs.ualberta.ca/~ruitong/files/cv_short.pdf">here</a>.</p>
<h2 class="code-line" data-line-start=10 data-line-end=11 ><a id="Experiences_10"></a>Experiences</h2>
<ul>
<li class="has-line-data" data-line-start="11" data-line-end="15"><strong>Research Team Lead</strong>, Borealis AI Lab (RBC Institute of Research)<br>
Nov. 2017 - present<br>
Research topics: reinforcement learning, adversarial robustness, and online learning<br>
Product projects: RL for algorithmic execution</li>
<li class="has-line-data" data-line-start="15" data-line-end="17"><strong>Research Intern</strong>, Borealis AI Lab (RBC Institute of Research)<br>
Jul. 2017 - Nov. 2017</li>
<li class="has-line-data" data-line-start="17" data-line-end="20"><strong>Research Intern</strong>, Amazon Alexa<br>
Summer 2015</li>
</ul>
<h2 class="code-line" data-line-start=20 data-line-end=21 ><a id="Education_20"></a>Education</h2>
<ul>
<li class="has-line-data" data-line-start="21" data-line-end="24"><strong>Ph.D student in Computing Science</strong>, University of Alberta<br>
Sept. 2010 - Sept. 2017<br>
Supervisors: <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> &amp; <a href="https://sites.ualberta.ca/~szepesva/index.html">Csaba Szepesvári</a></li>
<li class="has-line-data" data-line-start="24" data-line-end="27"><strong>Master of Mathematics in Computer Science</strong>, University of Waterloo<br>
Sept. 2008 - Jun. 2010<br>
Supervisor: <a href="https://cs.uwaterloo.ca/~mwg/">Mark Giesbrecht</a></li>
<li class="has-line-data" data-line-start="27" data-line-end="31"><strong>Bachelor of Mathematics</strong>, University of Science and Technology of China (USTC)<br>
Sept. 2004 - Jul. 2008<br>
Supervisor: <a href="http://staff.ustc.edu.cn/~xujm/">Junming Xu</a></li>
</ul>
<h2 class="code-line" data-line-start=31 data-line-end=32 ><a id="Publications_31"></a>Publications</h2>
<ul>
<li class="has-line-data" data-line-start="32" data-line-end="35"><strong>Max-margin adversarial (MMA) training: Direct input space margin maximization through adversarial training</strong><br>
GW. Ding, Y. Sharma, K. Lui, <strong><em>R. Huang</em></strong><br>
International Conference on Learning Representations (ICLR), 2020.</li>
<li class="has-line-data" data-line-start="35" data-line-end="38"><strong>Maximum Entropy Monte-Carlo Planning</strong><br>
C. Xiao, J. Mei, <strong><em>R. Huang</em></strong>, D. Schuurmans, M. Müller<br>
Neural Information Processing Systems (NIPS), 2019.</li>
<li class="has-line-data" data-line-start="38" data-line-end="41"><strong>On principled entropy exploration in policy optimization</strong><br>
C. Xiao, J. Mei, <strong><em>R. Huang</em></strong>, D. Schuurmans, M. Müller<br>
International Joint Conference on Artificial Intelligence (IJCAI), 2019.</li>
<li class="has-line-data" data-line-start="41" data-line-end="44"><strong>On the sensitivity of adversarial robustness to input data distributions</strong><br>
GW. Ding, K. Lui, X. Jin, L. Wang, <strong><em>R. Huang</em></strong><br>
International Conference on Learning Representations (ICLR), 2019.</li>
<li class="has-line-data" data-line-start="44" data-line-end="47"><strong>Dimensionality reduction has quantifiable imperfections: two geometric bounds</strong><br>
K. Lui, GW. Ding, <strong><em>R. Huang</em></strong>, R. McCann<br>
Neural Information Processing Systems (NIPS), 2018.</li>
<li class="has-line-data" data-line-start="47" data-line-end="50"><strong>Improving GAN training via binarized representation entropy (BRE) regularization</strong><br>
Y. Cao, GW. Ding, K. Lui, <strong><em>R. Huang</em></strong><br>
International Conference on Learning Representations (ICLR), 2018.</li>
<li class="has-line-data" data-line-start="50" data-line-end="53"><strong>Structured Best Arm Identification with Fixed Confidence.</strong><br>
<strong><em>R. Huang</em></strong>, M. Ajallooeian, C. Szepesvári, M. Müller<br>
International Conference on Algorithmic Learning Theory (ALT), 2017.</li>
<li class="has-line-data" data-line-start="53" data-line-end="57"><strong>Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities</strong><br>
<strong><em>R. Huang</em></strong>, T. Lattimore, A. György, C. Szepesvári<br>
Neural Information Processing Systems (NIPS), 2016.<br>
Journal version accepted to the Journal of Machine Learning Research (JMLR), 2017.</li>
<li class="has-line-data" data-line-start="57" data-line-end="60"><strong>Anchored Speech Detection</strong><br>
R. Maas, S.H.K. Parthasarathi, B. King, <strong><em>R. Huang</em></strong>, B. Hoffmeister<br>
Interspeech, 2016.</li>
<li class="has-line-data" data-line-start="60" data-line-end="63"><strong>Revise Saturated Activation Functions</strong><br>
B. Xu, <strong><em>R. Huang</em></strong>, M. Li<br>
<em>Workshop Track</em>, International Conference on Learning Representations (ICLR), 2016.</li>
<li class="has-line-data" data-line-start="63" data-line-end="66"><strong>Learning with a Strong Adversary</strong><br>
<strong><em>R. Huang</em></strong>, B. Xu, D. Schuurmans, C. Szepesvári<br>
CoRR, abs/1511.03034.</li>
<li class="has-line-data" data-line-start="66" data-line-end="69"><strong>Easy Data for Independent Component Analysis</strong><br>
<strong><em>R. Huang</em></strong>, A. György, C. Szepesvári<br>
Workshop “Learning Faster from Easy Data II” in Neural Information Processing Systems(NIPS), 2015.</li>
<li class="has-line-data" data-line-start="69" data-line-end="72"><strong>Deterministic Independent Component Analysis.</strong><br>
<strong><em>R. Huang</em></strong>, A. György, C. Szepesvári<br>
International Conference on Machine Learning (ICML), 2015.</li>
<li class="has-line-data" data-line-start="72" data-line-end="75"><strong>A Finite-Sample Generalization Bound for Semiparametric Regression: Partially Linear Models.</strong><br>
<strong><em>R. Huang</em></strong>, C. Szepesvári<br>
The 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014) 2014: 402-410.</li>
<li class="has-line-data" data-line-start="75" data-line-end="78"><strong>Generalization Bounds for Partially Linear Models.</strong><br>
<strong><em>R. Huang</em></strong>, C. Szepesvári<br>
Special session on Theory of Machine Learning, International Symposium on Artificial Intelligence and Mathematics (ISAIM-2014).</li>
<li class="has-line-data" data-line-start="78" data-line-end="82"><strong>Convex sparse coding, subspace learning, and semi-supervised extensions.</strong><br>
X. Zhang, Y. Yu, M. White, <strong><em>R. Huang</em></strong>, D. Schuurmans<br>
In Annual Conference on Artificial Intelligence (AAAI-2011), p567-573.</li>
</ul>
<h2 class="code-line" data-line-start=82 data-line-end=83 ><a id="Theses_82"></a>Theses</h2>
<ul>
<li class="has-line-data" data-line-start="84" data-line-end="85">Instance-dependent analysis of learning algorithms. (PhD Thesis) <a href="https://era.library.ualberta.ca/files/c2f75r839s#.Wn8xRVQ-eYU">pdf</a></li>
<li class="has-line-data" data-line-start="85" data-line-end="86">Decomposition of Finite-Dimensional Matrix Algebras over $F_q(y)$. （Master Thesis）<a href="https://uwspace.uwaterloo.ca/bitstream/handle/10012/5360/4uw-ethesis.pdf?sequence=1">pdf</a></li>
<li class="has-line-data" data-line-start="86" data-line-end="88">Decreasing the Diameter of Bounded Degree Graphs. (Bacherlor Thesis)</li>
</ul>
<h2 class="code-line" data-line-start=88 data-line-end=89 ><a id="Awards_and_Honors_88"></a>Awards and Honors</h2>
<p class="has-line-data" data-line-start="89" data-line-end="95">PhD outstanding thesis award (runner up), 2017<br>
Provost Doctoral Entrance Award, University of Alberta, 2010&amp;2011<br>
Graduate Entrance Scholarship, University of Waterloo, 2008<br>
Outstanding Undergraduate Thesis Award (3/75), USTC, 2008<br>
National Scholarship (honored by China Ministry of Education), USTC, 2007<br>
China Aerospace Science and Technology Corporation Scholarship, USTC, 2006</p>
<h2 class="code-line" data-line-start=96 data-line-end=97 ><a id="Services_96"></a>Services</h2>
<p class="has-line-data" data-line-start="98" data-line-end="99">Reviewer:</p>
<ul>
<li class="has-line-data" data-line-start="100" data-line-end="101">Conferences: AISTATS, ICML, ALT, COLT, NIPS, IJCAI, ICLR, AAAI, ACML, UAI</li>
<li class="has-line-data" data-line-start="101" data-line-end="103">Journals: Journal of Machine Learning Research, Annals of Applied Statistics, Machine Learning, Neurocomputing, Neural Networks</li>
</ul>
<h2 class="code-line" data-line-start=103 data-line-end=104 ><a id="Teachings_103"></a>Teachings</h2>
<p class="has-line-data" data-line-start="105" data-line-end="111">Teaching assistant at the University of Alberta for the following courses:<br>
CMPUT 101: Introduction to Computing<br>
CMPUT 210: Codes, Codemakers, Codebreakers: An Introduction to Cryptography<br>
CMPUT 272: Formal Systems and Logic in Computing Science<br>
CMPUT 304: Algorithms II<br>
CMPUT 466/551: Machine Learning</p>

</body></html>